#!/usr/bin/env python3
"""Analisa resultados do Locust em uma pasta e gera gráficos e um resumo.

Uso:
  python analyze_results_dir.py /caminho/para/results/run1 --out out_dir

O script procura por arquivos com sufixos:
  *_requests.csv, *_stats_history.csv, *_failures.csv

Gera gráficos em PNG no diretório de saída e imprime um resumo no stdout.
"""
from pathlib import Path
import argparse
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import sys
import re
import numpy as np


def find_file(folder: Path, pattern: str):
    files = list(folder.glob(pattern))
    return files[0] if files else None


def robust_column(df, contains):
    for c in df.columns:
        if contains.lower() in c.lower():
            return c
    return None


def analyze_stats_history(stats_csv: Path, outdir: Path):
    df = pd.read_csv(stats_csv)
    df.columns = [c.strip() for c in df.columns]
    # drop first data row which may be incomplete
    if len(df) > 0:
        df = df.iloc[1:].reset_index(drop=True)
    print(f"[analyze_stats_history] detected columns: {df.columns.tolist()}")
    cols = df.columns.tolist()
    # Guess time column
    time_col = next((c for c in cols if 'timestamp' in c.lower() or 'time'==c.lower() or 'date' in c.lower()), None)
    # Guess rps and median
    rps_col = next((c for c in cols if 'total_rps' in c.lower() or 'requests/s' in c.lower() or 'requests_per_second' in c.lower()), None)
    median_col = next((c for c in cols if 'median' in c.lower() or 'median_response' in c.lower()), None)
    # avg_col not used directly here; we focus on rps and median columns

    # Try convert time
    if time_col:
        try:
            df[time_col] = pd.to_datetime(df[time_col], unit='s')
        except Exception:
            try:
                df[time_col] = pd.to_datetime(df[time_col])
            except Exception:
                pass

    out_files = []
    fig, ax1 = plt.subplots(figsize=(12,6))
    plotted = False

    # Plot RPS on primary (left) axis if available
    if rps_col and time_col in df.columns:
        sns.lineplot(ax=ax1, x=time_col, y=rps_col, data=df, label='RPS (req/s)', color='C0', legend=False)
        ax1.set_ylabel('Requests/s', color='k')
        plotted = True

    # Plot median on secondary (right) axis if available
    ax2 = None
    if median_col and time_col in df.columns:
        ax2 = ax1.twinx()
        sns.lineplot(ax=ax2, x=time_col, y=median_col, data=df, label='Median RT (ms)', color='C1', legend=False)
        ax2.set_ylabel('Median RT (ms)', color='k')
        plotted = True

    if plotted:
        # Grid and styling
        ax1.set_axisbelow(True)
        ax1.yaxis.grid(True, linestyle='--', alpha=0.4)
        ax1.xaxis.grid(True, linestyle='--', alpha=0.2)

        # X-axis label (time/index)
        ax1.set_xlabel('Time' if time_col else 'Index')

        # Color y-axis ticks and spines to match the plotted lines
        ax1.tick_params(axis='y', colors='k')
        try:
            ax1.spines['left'].set_color('k')
        except Exception:
            pass
        if ax2 is not None:
            ax2.tick_params(axis='y', colors='k')
            try:
                from collections import OrderedDict
                legend_map = OrderedDict()
                for handle, label in zip(handles, labels):
                    if label not in legend_map:
                        legend_map[label] = handle
                if ax2 is not None:
                    for handle, label in zip(h2, l2):
                        if label not in legend_map:
                            legend_map[label] = handle

                # Place legend outside the plot on the right to avoid overlap
                ax1.legend(list(legend_map.values()), list(legend_map.keys()), loc='upper left', bbox_to_anchor=(1.02, 1), borderaxespad=0., framealpha=0.9)
            except Exception:
                pass

        # Combine legends from both axes and place inside plot
        handles1, labels1 = ax1.get_legend_handles_labels()
        handles = handles1.copy()
        labels = labels1.copy()
        if ax2 is not None:
            h2, l2 = ax2.get_legend_handles_labels()
            # append only labels that are not already present (deduplicate)
            for h, l in zip(h2, l2):
                if l not in labels:
                    handles.append(h)
                    labels.append(l)
        ax1.legend(handles, labels, loc='upper left', framealpha=0.9)

        plt.title('Throughput and median response over time')
        plt.tight_layout()
        p = outdir / f"{stats_csv.stem}_timeseries.png"
        plt.savefig(p)
        out_files.append(p)
        plt.close(fig)

    # Only timeseries plot is generated by this analyzer (keeps output minimal)

    # Compute simple stats
    stats = {}
    if rps_col and rps_col in df.columns:
        stats['avg_rps'] = float(df[rps_col].mean())
        stats['max_rps'] = float(df[rps_col].max())
    if median_col and median_col in df.columns:
        stats['median_rt_mean'] = float(df[median_col].mean())

    return stats, out_files


def analyze_stats_per_endpoint(stats_csv: Path, outdir: Path):
    """Analyze per-endpoint aggregated stats from *_stats.csv and generate plots.

    Produces:
      - top_avg_response.png: top endpoints by average response (top 20)
      - top_request_count.png: top endpoints by request count (top 20)
      - avg_vs_requests.png: scatter plot avg response vs request count
      - percentiles_top10.png: grouped percentiles (if percentile columns exist)
    """
    df = pd.read_csv(stats_csv)
    df.columns = [c.strip() for c in df.columns]
    out_files = []

    name_col = None
    for candidate in ['Name', 'name', 'Request', 'request']:
        if candidate in df.columns:
            name_col = candidate
            break
    if name_col is None:
        return {}, out_files

    # Normalize endpoint paths into logical groups
    def normalize_endpoint(path: str) -> str:
        if not isinstance(path, str) or not path.startswith('/'):
            return path
        parts = path.split('/')
        norm_parts = []
        for p in parts:
            if p == '':
                continue
            if re.fullmatch(r"\d+", p):
                norm_parts.append('{id}')
                continue
            if re.fullmatch(r"[A-Za-z0-9\-_:]{4,}", p) and not re.fullmatch(r"[A-Za-z]+", p):
                norm_parts.append('{id}')
                continue
            norm_parts.append(p)
        return '/' + '/'.join(norm_parts)

    df['endpoint_group'] = df[name_col].apply(normalize_endpoint)

    # detect percentile columns and request count column
    pct_cols = []
    for candidate in ['50%', '75%', '90%', '99%', '99.9%']:
        c = robust_column(df, candidate)
        if c and c in df.columns:
            pct_cols.append((candidate, c))

    reqs_col = robust_column(df, 'request count') or robust_column(df, 'requests/s') or robust_column(df, 'requests')
    avg_col = robust_column(df, 'average response') or robust_column(df, 'average') or robust_column(df, 'avg')

    # aggregate by endpoint_group and compute weighted percentile averages using request counts
    group_key = 'endpoint_group'
    grp = df.copy()
    if reqs_col and reqs_col in grp.columns:
        grp[reqs_col] = pd.to_numeric(grp[reqs_col], errors='coerce').fillna(0)
        agg_reqs = grp.groupby(group_key)[reqs_col].sum()
    else:
        agg_reqs = None

    pct_agg = {}
    try:
        if pct_cols and agg_reqs is not None and not agg_reqs.empty:
            # compute weighted percentile aggregates per group
            for label, colname in pct_cols:
                vals = (grp[colname].astype(float) * grp[reqs_col]).groupby(grp[group_key]).sum()
                pct_agg[label] = (vals / agg_reqs).replace([np.inf, -np.inf], np.nan)
            pct_df = pd.DataFrame(pct_agg)

            # compute average per group (weighted by requests if possible)
            if avg_col and avg_col in grp.columns and agg_reqs is not None:
                avg_group = (grp[avg_col].astype(float) * grp[reqs_col]).groupby(grp[group_key]).sum() / agg_reqs
            elif avg_col and avg_col in grp.columns:
                avg_group = grp.groupby(group_key)[avg_col].mean()
            else:
                avg_group = None

            # choose top groups by average response time if available, otherwise by request count
            if avg_group is not None:
                top_groups = avg_group.sort_values(ascending=False).head(10).index.tolist()
            else:
                top_groups = agg_reqs.sort_values(ascending=False).head(10).index.tolist()

            # restrict and order pct_df by top_groups
            pct_df = pct_df.reindex(top_groups).dropna(how='all')

            # include average in the plot if available
            if avg_group is not None:
                pct_df['avg'] = avg_group.reindex(pct_df.index).astype(float)

            if not pct_df.empty:
                # order columns so percentiles come first, avg last
                cols = [c for c in pct_df.columns if c != 'avg'] + (['avg'] if 'avg' in pct_df.columns else [])
                pct_df = pct_df[cols]
                # invert order (other direction) so the highest/lowest appear as requested
                pct_df = pct_df.iloc[::-1]
                ax = pct_df.plot(kind='bar', figsize=(12,6))
                # put grid behind bars
                ax.set_axisbelow(True)
                ax.yaxis.grid(True, linestyle='--', alpha=0.4)
                plt.ylabel('ms')
                plt.title('Percentiles and average for top endpoint groups')
                plt.tight_layout()
                p = outdir / f"{stats_csv.stem}_percentiles_top_10.png"
                plt.savefig(p)
                out_files.append(p)
                plt.close()
    except Exception:
        pass

    summary = {
        'endpoints': int(df.shape[0]),
        'endpoint_groups': int(df[group_key].nunique())
    }
    return summary, out_files


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('folder', type=Path, help='Folder with Locust CSV files')
    parser.add_argument('--out', type=Path, help='Output folder for graphs (defaults to <folder>/analysis)', default=None)
    args = parser.parse_args()

    folder = args.folder
    if not folder.exists() or not folder.is_dir():
        print('Folder not found:', folder)
        sys.exit(1)

    outdir = args.out or (folder / 'analysis')
    outdir.mkdir(parents=True, exist_ok=True)

    results = {}
    generated = []

    stats_file = find_file(folder, '*_stats_history.csv')
    stats_per_endpoint_file = find_file(folder, '*_stats.csv')

    found = []
    if stats_file:
        found.append(stats_file.name)
    if stats_per_endpoint_file:
        found.append(stats_per_endpoint_file.name)
    

    if found:
        print('Found files:', ', '.join(found))
    else:
        print('No recognized Locust CSV files found in', folder)
        sys.exit(0)

    # Analyze stats_history (timeseries)
    if stats_file:
        stats, files = analyze_stats_history(stats_file, outdir)
        results['stats_history'] = stats
        generated += files

    # Analyze per-endpoint aggregated stats (if present)
    if stats_per_endpoint_file:
        psum, pfiles = analyze_stats_per_endpoint(stats_per_endpoint_file, outdir)
        results['per_endpoint'] = psum
        generated += pfiles

    # (failures analysis not run) -- user requested only timeseries and percentiles by group

    # Print summary
    print('\nAnalysis summary:')
    for k, v in results.items():
        print(f"- {k}: {v}")

    if generated:
        print(f"\nGenerated {len(generated)} plot(s) in: {outdir}")
        for p in generated:
            print('  -', p)
    else:
        print('\nNo plots generated.')


if __name__ == '__main__':
    main()
